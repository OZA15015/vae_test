/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
56000
14000
Epoch: 0 train_loss: 195.4381103515625
Epoch: 0 test_loss: 169.96115112304688
Epoch: 1 train_loss: 166.41757202148438
Epoch: 1 test_loss: 162.39352416992188
Epoch: 2 train_loss: 160.67222595214844
Epoch: 2 test_loss: 157.6856231689453
Epoch: 3 train_loss: 156.37472534179688
Epoch: 3 test_loss: 154.843505859375
Epoch: 4 train_loss: 153.64808654785156
Epoch: 4 test_loss: 152.19361877441406
Epoch: 5 train_loss: 151.57179260253906
Epoch: 5 test_loss: 150.59921264648438
Epoch: 6 train_loss: 150.0638885498047
Epoch: 6 test_loss: 149.3518524169922
Epoch: 7 train_loss: 148.84413146972656
Epoch: 7 test_loss: 148.19467163085938
Epoch: 8 train_loss: 147.75901794433594
Epoch: 8 test_loss: 147.0410614013672
Epoch: 9 train_loss: 146.95753479003906
Epoch: 9 test_loss: 146.9127960205078
Epoch: 10 train_loss: 146.25437927246094
Epoch: 10 test_loss: 146.10177612304688
Epoch: 11 train_loss: 145.6261444091797
Epoch: 11 test_loss: 146.26348876953125
Epoch: 12 train_loss: 145.1414031982422
Epoch: 12 test_loss: 145.4661865234375
Epoch: 13 train_loss: 144.72897338867188
Epoch: 13 test_loss: 144.91908264160156
Epoch: 14 train_loss: 144.0980224609375
Epoch: 14 test_loss: 144.521240234375
Epoch: 15 train_loss: 143.6446990966797
Epoch: 15 test_loss: 144.16696166992188
Epoch: 16 train_loss: 143.34814453125
Epoch: 16 test_loss: 144.01766967773438
Epoch: 17 train_loss: 142.96682739257812
Epoch: 17 test_loss: 144.02972412109375
Epoch: 18 train_loss: 142.5939178466797
Epoch: 18 test_loss: 143.22747802734375
Epoch: 19 train_loss: 142.21011352539062
Epoch: 19 test_loss: 143.57655334472656
Epoch: 20 train_loss: 142.03160095214844
Epoch: 20 test_loss: 142.8114013671875
Epoch: 21 train_loss: 141.83172607421875
Epoch: 21 test_loss: 142.6881561279297
Epoch: 22 train_loss: 141.53695678710938
Epoch: 22 test_loss: 142.60116577148438
Epoch: 23 train_loss: 141.4464569091797
Epoch: 23 test_loss: 142.3307647705078
Epoch: 24 train_loss: 141.13780212402344
Epoch: 24 test_loss: 142.50035095214844
Epoch: 25 train_loss: 140.94749450683594
Epoch: 25 test_loss: 141.92726135253906
Epoch: 26 train_loss: 140.6503143310547
Epoch: 26 test_loss: 142.2679901123047
Epoch: 27 train_loss: 140.51016235351562
Epoch: 27 test_loss: 141.98887634277344
Epoch: 28 train_loss: 140.1676788330078
Epoch: 28 test_loss: 141.75901794433594
Epoch: 29 train_loss: 140.0945281982422
Epoch: 29 test_loss: 141.86207580566406
Epoch: 30 train_loss: 139.88580322265625
Epoch: 30 test_loss: 141.5615997314453
Epoch: 31 train_loss: 140.12181091308594
Epoch: 31 test_loss: 141.6222381591797
Epoch: 32 train_loss: 139.6934051513672
Epoch: 32 test_loss: 141.66525268554688
Epoch: 33 train_loss: 139.6084442138672
Epoch: 33 test_loss: 141.41404724121094
Epoch: 34 train_loss: 139.62098693847656
Epoch: 34 test_loss: 141.9740753173828
Epoch: 35 train_loss: 139.51181030273438
Epoch: 35 test_loss: 141.4477996826172
Epoch: 36 train_loss: 139.0646514892578
Epoch: 36 test_loss: 140.98876953125
Epoch: 37 train_loss: 139.1498260498047
Epoch: 37 test_loss: 140.81204223632812
Epoch: 38 train_loss: 138.76377868652344
Epoch: 38 test_loss: 141.03662109375
Epoch: 39 train_loss: 139.0034637451172
Epoch: 39 test_loss: 140.74371337890625
Epoch: 40 train_loss: 138.5892791748047
Epoch: 40 test_loss: 140.77670288085938
Epoch: 41 train_loss: 138.84947204589844
Epoch: 41 test_loss: 140.60067749023438
Epoch: 42 train_loss: 138.42616271972656
Epoch: 42 test_loss: 140.83204650878906
Epoch: 43 train_loss: 138.2709197998047
Epoch: 43 test_loss: 140.5486602783203
Epoch: 44 train_loss: 138.24241638183594
Epoch: 44 test_loss: 140.67227172851562
Epoch: 45 train_loss: 138.20333862304688
Epoch: 45 test_loss: 140.6122283935547
Epoch: 46 train_loss: 138.07806396484375
Epoch: 46 test_loss: 140.5384979248047
Epoch: 47 train_loss: 138.28564453125
Epoch: 47 test_loss: 140.46730041503906
Epoch: 48 train_loss: 137.97947692871094
Epoch: 48 test_loss: 140.27944946289062
Epoch: 49 train_loss: 137.74603271484375
Epoch: 49 test_loss: 141.5965576171875
